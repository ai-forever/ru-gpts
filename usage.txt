conda activate rugpt
#export CUDA_VISIBLE_DEVICES=2
#sudo apt install gcc-8 g++-8 
#sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 80 --slave /usr/bin/g++ g++ /usr/bin/g++-8 --slave /usr/bin/gcov gcov /usr/bin/gcov-8
#then install apex

python generate_transformers.py \
    --model_type=gpt2 \
    --model_name_or_path=$HOME/gpt2_large_bbpe_v50 \
    --k=10 \
    --p=0.9

python pretrain_transformers.py \
    --block_size=1023 \
    --train_data_file=$HOME/gpt_text/pelevin_train.txt \
    --output_dir=$HOME/gpt2_large_bbpe_v50 \
    --model_type=gpt2 \
    --model_name_or_path=$HOME/gpt2_large_bbpe_v50 \
    --do_eval \
    --eval_data_file=$HOME/gpt_text/tolstoy_valid.txt \
    --fp16

# perplexity = tensor(14.0039)    

python pretrain_transformers.py \
    --block_size=1023 \
    --train_data_file=$HOME/gpt_text/pelevin_train.txt \
    --output_dir=$HOME/gpt2_large_bbpe_v50 \
    --model_type=gpt2 \
    --model_name_or_path=$HOME/gpt2_large_bbpe_v50 \
    --do_eval \
    --eval_data_file=$HOME/gpt_text/pelevin_valid.txt \
    --fp16

# perplexity = tensor(18.5913)

python pretrain_transformers.py \
    --block_size=1023 \
    --output_dir=$HOME/gpt2_large_bbpe_v50/pelevin \
    --model_type=gpt2 \
    --model_name_or_path=$HOME/gpt2_large_bbpe_v50 \
    --do_train \
    --train_data_file=$HOME/gpt_text/pelevin_train.txt \
    --do_eval \
    --eval_data_file=$HOME/gpt_text/pelevin_valid.txt \
    --num_train_epochs=1 \
    --per_gpu_train_batch_size=1 \
    --gradient_accumulation_steps=4 \
    --fp16 \
    --warmup_steps 25 \
    --evaluate_during_training \
    --logging_steps=100 \
    --learning_rate=1e-5

# perplexity = tensor(16.1595)

python pretrain_transformers.py \
    --block_size=1023 \
    --output_dir=$HOME/gpt2_large_bbpe_v50/poetry \
    --model_type=gpt2 \
    --model_name_or_path=$HOME/gpt2_large_bbpe_v50 \
    --do_train \
    --train_data_file=$HOME/gpt_text/poetry_train.txt \
    --do_eval \
    --eval_data_file=$HOME/gpt_text/poetry_valid.txt \
    --num_train_epochs=1 \
    --per_gpu_train_batch_size=1 \
    --gradient_accumulation_steps=4 \
    --fp16 \
    --warmup_steps 25 \
    --evaluate_during_training \
    --logging_steps=100 \
    --learning_rate=1e-5

# perplexity = tensor(23.8015)