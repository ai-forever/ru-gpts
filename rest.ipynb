{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.11 64-bit ('rugpt': conda)",
   "display_name": "Python 3.6.11 64-bit ('rugpt': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b5deb69299e72b4c1f8359a6e9c7a994a85d335c063da042ceb38c2473c9a8bd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "device = environ.get('DEVICE', 'cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = environ.get('MODEL', 'poetry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flavor_id = device + environ.get('INSTANCE', ':0')\n",
    "from tendo import singleton\n",
    "me = singleton.SingleInstance(flavor_id=flavor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(filename=f\"logs/{hash(flavor_id)}.log\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n\nDefaults for this optimization level are:\nenabled                : True\nopt_level              : O2\ncast_model_type        : torch.float16\npatch_torch_functions  : False\nkeep_batchnorm_fp32    : True\nmaster_weights         : True\nloss_scale             : dynamic\nProcessing user overrides (additional kwargs that are not None)...\nAfter processing overrides, optimization options are:\nenabled                : True\nopt_level              : O2\ncast_model_type        : torch.float16\npatch_torch_functions  : False\nkeep_batchnorm_fp32    : True\nmaster_weights         : True\nloss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "model = amp.initialize(model, opt_level='O2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(prompt, length:int, num_samples:int, allow_linebreak:bool):\n",
    "    logger.info(prompt)\n",
    "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "    bad_words_ids = [tokenizer.encode('[')[0], tokenizer.encode('(')[0], tokenizer.encode('1\\xa01')[1]]\n",
    "    linebreak = tokenizer.encode(\"1\\n1\")[1]\n",
    "    bad_words_ids += [] if allow_linebreak else [linebreak]\n",
    "    bad_words_ids = [[b] for b in bad_words_ids] + [[linebreak,linebreak]]\n",
    "    output_sequences = model.generate(\n",
    "            input_ids=encoded_prompt,\n",
    "            max_length=length + len(encoded_prompt[0]),\n",
    "            temperature=1,\n",
    "            top_k=0,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,num_return_sequences=num_samples,\n",
    "            bad_words_ids = bad_words_ids\n",
    "        )\n",
    "    \n",
    "    if len(output_sequences.shape) > 2:\n",
    "            output_sequences.squeeze_()\n",
    "    generated_sequences = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        total_sequence = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "        generated_sequences.append(total_sequence)\n",
    "\n",
    "    reg_text = [re.match(r'[\\w\\W]*[\\.!?]\\n', item) for item in generated_sequences]\n",
    "    reg_text2 = [re.match(r'[\\w\\W]*[\\.!?]', item) for item in generated_sequences]\n",
    "    result = [reg_item[0] if reg_item else reg_item2[0] if reg_item2 else item for reg_item, reg_item2, item in zip(reg_text, reg_text2, generated_sequences)]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[' мордой! не мог на меня не нападать! Ну, да Бог тебя простит!»— сказал премудрый Поп. И — поволок, повесив за ногу, а мудрец — утешал царя: «Потерпи, Ванька, потерпи!',\n",
       " ' шкуры. Против меня? Зачем? На то есть люди иные. Для тебя. Куда ты?! Не упорствуй. Без брони - и твой меч не страшнее кинжала. По-другому. Я ж говорю - лев ты - и позови! Кто?',\n",
       " ' крови, но здесь ты – дикий лев. Не бойся, друг мой, я приручу тебя. Я, быть может, больше устану, чем ты, но я не отступлюсь и поборю тебя.',\n",
       " ', и в высоте ты орел, и во внутренности ты червь: ты долгонос, ты змиен, дух крылатый, ты чарами и мерзостями меняешь знаменья небес и земли; но пером единым ты нарисован, но законом']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "get_sample('на словах ты лев толстой', 50, 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from starlette.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "import threading\n",
    "\n",
    "app = FastAPI(title=\"Russian GPT-2\", version=\"0.1\",)\n",
    "app.add_middleware(\n",
    "        CORSMiddleware,\n",
    "        allow_origins=[\"*\"],\n",
    "        allow_credentials=True,\n",
    "        allow_methods=[\"*\"],\n",
    "        allow_headers=[\"*\"],\n",
    "    )\n",
    "\n",
    "lock = threading.RLock()\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    prompt:str = Field(..., max_length=3000, title='Model prompt')\n",
    "    length:int = Field(15, ge=1, le=60, title='Number of tokens generated in each sample')\n",
    "    num_samples:int = Field(3, ge=1, le=5, title='Number of samples generated')\n",
    "    allow_linebreak:bool = Field(False, title='Allow linebreak in a sample')\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "def gen_sample(prompt: Prompt):\n",
    "    with lock:\n",
    "        return {\"replies\": get_sample(prompt.prompt, prompt.length, prompt.num_samples, prompt.allow_linebreak)}\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def healthcheck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}