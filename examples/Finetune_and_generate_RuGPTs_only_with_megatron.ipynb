{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetune_and_generate_RuGPTs_only_with_megatron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRzAXrPVzsHX"
      },
      "source": [
        "# Finetune RuGPTs in megatron without deepspeed\n",
        "How to finetune RuGPTs models with megatron without deepspeed. Example for RuGPT3Small. Note for other models it will take more GPU memory.\n",
        "\n",
        "This notebook is valid for all RuGPTs models except RuGPT3XL.\n",
        "## Install env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu1OzWZ6zqQv"
      },
      "source": [
        "!pip3 install transformers==3.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozJOYbK-11pk",
        "outputId": "47a4a5b0-71c8-46dc-b9df-4f1cea205ea1"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46Pk6DJ19Jk"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gE7xBM_z-uW"
      },
      "source": [
        "!git clone  https://github.com/sberbank-ai/ru-gpts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8di4sCoS0Pyw"
      },
      "source": [
        "## Download files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qG_A1n0CiF"
      },
      "source": [
        "!wget -O train.txt https://www.dropbox.com/s/oa3v9c7g9bp40xw/train.txt?dl=0\n",
        "!wget -O valid.txt https://www.dropbox.com/s/mworl3ld6r3bg62/valid.txt?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqW38Hni64xH"
      },
      "source": [
        "## Prepare data for parallel\n",
        "We use custom implementation of distributed dataset. For training and evaluating we should specify file `file.list` with list of paths to txt files. All files from `file.list` will be splitted between aviable GPUs. The logic of splitting is described by the following code:\n",
        "\n",
        "```python\n",
        "shard_size = len(files) // world_size\n",
        "shard_start = rank * shard_size\n",
        "shard_end = (rank + 1) * shard_size\n",
        "files = files[shard_start:shard_end]\n",
        "```\n",
        "\n",
        "For more details please see full code of dataset: `src.dataset_rugpt3.RuGpt3TextDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtItuLGA38db"
      },
      "source": [
        "!echo train.txt > train.list\n",
        "!echo valid.txt > valid.list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EF0JepF0S41"
      },
      "source": [
        "## Train\n",
        "Load model from Huggingface and finetune on essays.\n",
        "\n",
        "This will take arount ten minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHluAlFh0SJo"
      },
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!python ru-gpts/pretrain_gpt3.py \\\n",
        "  --train-data-path \"train.list\" \\\n",
        "  --test-data-path \"valid.list\" \\\n",
        "  --max-files-per-process 100 \\\n",
        "  --logging-dir=\"log\" \\\n",
        "  --save model \\\n",
        "  --load-huggingface sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --save-interval 1000 \\\n",
        "  --log-interval 100 \\\n",
        "  --eval-interval 1000 \\\n",
        "  --eval-iters 100 \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 12 \\\n",
        "  --hidden-size 768 \\\n",
        "  --num-attention-heads 12 \\\n",
        "  --batch-size 1 \\\n",
        "  --seq-length 512 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --train-iters 2000 \\\n",
        "  --resume-dataloader \\\n",
        "  --distributed-backend \"nccl\" \\\n",
        "  --lr 0.00015 \\\n",
        "  --lr-decay-style \"cosine\" \\\n",
        "  --lr-decay-iters 3200 \\\n",
        "  --clip-grad 0.5 \\\n",
        "  --warmup .004\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALvcD5SE8RtP"
      },
      "source": [
        "At the end of training output should be something like this:\n",
        "\n",
        "\"-----------------------------------------------------------------------------------------\n",
        "\n",
        " validation loss at the end of training for test data | LM loss: 2.7927 | LM PPL: 16.325\n",
        " \n",
        "-----------------------------------------------------------------------------------------\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HmKilrb8lQm"
      },
      "source": [
        "## Generate\n",
        "\n",
        "Load pretrained model from dir and generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAH-WpCG8lmG",
        "outputId": "0e0cd69a-8e4e-4463-91ed-c2fce32b75ae"
      },
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!python ru-gpts/generate_samples.py \\\n",
        "  --load model/ \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 12 \\\n",
        "  --hidden-size 768 \\\n",
        "  --num-attention-heads 12 \\\n",
        "  --batch-size 1 \\\n",
        "  --seq-length 512 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --distributed-backend \"nccl\" \\\n",
        "  --tokenizer-path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --no-load-optim\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-11 21:33:07.459111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Generate Samples\n",
            "WARNING: No training data specified\n",
            "using world size: 1 and model-parallel size: 1 \n",
            " > using dynamic loss scaling\n",
            "> initializing model parallel with size 1\n",
            "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
            "prepare tokenizer done, size 50264\n",
            "building GPT3 model ...\n",
            " > number of parameters on model parallel rank 0: 125231616\n",
            "Load checkpoint from model/\n",
            "global rank 0 is loading checkpoint model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "  successfully loaded model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "Loaded\n",
            "\n",
            "Context prompt (stop to exit) >>> <s>Тема: «Создает человека природа, но развивает и образует его общество». (В.Т. Белинский)\\nСочинение: \n",
            "\n",
            "\u001b[H\u001b[2J\n",
            "Taken time 15.56\n",
            "\n",
            "\n",
            "Context: <s>Тема: «Создает человека природа, но развивает и образует его общество». (В.Т. Белинский)\\nСочинение: \n",
            "\n",
            "GPT: <s>Тема: «Создает человека природа, но развивает и образует его общество». (В.Т. Белинский)\\nСочинение:  С. Паркинсон - американский публицист, политический деятель. Он понимает, что политика - это выбор между гибельным и неприятным. Как часто приходится политикам делать этот выбор. Порой, действительно, трудно обманывать ожидания своих избирателей, народа, но это бывает необходимо. Как сказал автор высказывания, чтобы власть не обманула ожидания народа. Я согласен с автором. Люди, получившие в результате неправильный выбор, часто забывают о тех, кто живет на другом конце земного шара. Угасая, они забывают о том, что на планете существуют «статусные» организации, которые могут нарушить правила человеческого бытия. Это организации, которые на терпящем бедствие корабле спасают в первую очередь женщин и детей, а потом спасаются сами. Это страны, в которых не выполняется функция «расширения территории» (вооруженные силы, милиция, пограничная стража). В демократических странах сейчас действует многопартийная система, которая соответствует международным документам о политических правах человека. Система политического права – многопартийная. Она подразумевает, что каждый гражданин несет ответственность за свой выбор, и потому каждый должен ориентироваться в политике, быть активным участником общественной жизни страны. Из философско-правовых понятий « политика» и « права» известно, что политика - это выбор между гибельным и неприятным. Как часто приходится политикам делать этот выбор. Порой, действительно, трудно обманывать ожидания своих избирателей, народа, но это бывает необходимо. Из других философских понятий: «Политика<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "\n",
            "Context prompt (stop to exit) >>> Prompt should not be empty!\n",
            "\n",
            "Context prompt (stop to exit) >>> Traceback (most recent call last):\n",
            "  File \"ru-gpts/generate_samples.py\", line 193, in <module>\n",
            "    main()\n",
            "  File \"ru-gpts/generate_samples.py\", line 189, in main\n",
            "    generate_samples(model, tokenizer, args)\n",
            "  File \"ru-gpts/generate_samples.py\", line 98, in generate_samples\n",
            "    raw_text = input(\"\\nContext prompt (stop to exit) >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCapfDfeBq0x"
      },
      "source": [
        "### Convert checkpoint to Huggingface format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JnhIyqd-Eeo",
        "outputId": "2b70974b-6d4a-455d-9a95-0084bf34acb3"
      },
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!python ru-gpts/convert2huggingface.py \\\n",
        "  --load model/ \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 12 \\\n",
        "  --hidden-size 768 \\\n",
        "  --num-attention-heads 12 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --tokenizer-path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --no-load-optim \\\n",
        "  --export-huggingface model_hf\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-11 21:49:23.948606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "WARNING: No training data specified\n",
            "using world size: 1 and model-parallel size: 1 \n",
            " > using dynamic loss scaling\n",
            "> initializing model parallel with size 1\n",
            "prepare tokenizer done, size 50264\n",
            "building GPT3 model ...\n",
            " > number of parameters on model parallel rank 0: 125231616\n",
            "Load checkpoint from model/\n",
            "global rank 0 is loading checkpoint model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "  successfully loaded model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "Loaded\n",
            "Export to huggingface model  model_hf with config {'vocab_size': 50264, 'n_positions': 2048, 'n_ctx': 2048, 'n_embd': 768, 'n_layer': 12, 'n_head': 12}\n",
            "Saved huggingface model <class 'src.model.distributed.DistributedDataParallel'>\n",
            "Exported in huggingface format to model_hf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIfhv7KvDKQi",
        "outputId": "6c51866e-377e-4da6-99b9-b30ded2f67db"
      },
      "source": [
        "!ls model_hf"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.json  pytorch_model.bin\t      tokenizer_config.json\n",
            "merges.txt   special_tokens_map.json  vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRlEwlPdE0L8"
      },
      "source": [
        "#### Test load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U81i24aEEm0"
      },
      "source": [
        "from transformers import GPT2LMHeadModel"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBRatZnJEcCX"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"model_hf\")"
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}